{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c54360f",
   "metadata": {},
   "source": [
    "# Content-Based Recommendation System using BERT\n",
    "\n",
    "## 1. Objectives & Business Value\n",
    "This notebook implements **Content-Based Recommendation Systems** leveraging **BERT embeddings** to understand product nuances (Brand, Category, Price).\n",
    "\n",
    "### Core Components:\n",
    "1.  **User-Based Content Recommender:** personalized suggestions based on a user's interaction history profile.\n",
    "2.  **Brand/Item-Based Recommender:** \"More like this\" suggestions based on brand and price similarity.\n",
    "\n",
    "### Business Value:\n",
    "* **Discoverability:** Enables users to find alternative products within the same brand ecosystem.\n",
    "* **Cold Start:** Provides recommendations for products based on metadata even if interaction data is sparse.\n",
    "\n",
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64732d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "\n",
    "# GPU Check\n",
    "from tensorflow.python.client import device_lib\n",
    "print(f\"GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "\n",
    "# Set Seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955fa221",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Feature Engineering\n",
    "We load the preprocessed master dataset and generate textual features for BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67885e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc3553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "print (\"No of GPUS present:\",len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e60eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Clean Data\n",
    "data = pd.read_csv(\"Master_Ecommerce_Dataset.csv\")\n",
    "\n",
    "# Fill missing values for text generation\n",
    "data['brand'] = data['brand'].fillna('unknown')\n",
    "data['category_code'] = data['category_code'].fillna('unknown')\n",
    "\n",
    "# Normalize Price for Embedding\n",
    "scaler = MinMaxScaler()\n",
    "data['price_norm'] = scaler.fit_transform(data[['price']])\n",
    "\n",
    "\n",
    "data['text'] = (\n",
    "    data['brand'] + \" \" + \n",
    "    data['category_code'] + \" price \" + \n",
    "    data['price_norm'].astype(str)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Initialize BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embeddings(texts, batch_size=64):\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Process in batches to manage memory\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = [str(x) for x in texts[i:i+batch_size]]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=32, \n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "        \n",
    "        outputs = bert_model(**inputs)\n",
    "        # Extract CLS token (contextual representation)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        all_embeddings.append(cls_embeddings.numpy())\n",
    "        \n",
    "        if i % (batch_size * 10) == 0:\n",
    "            print(f\"Processed {i}/{len(texts)}\", end='\\r')\n",
    "            \n",
    "    return np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "# Generate Embeddings\n",
    "print(\"Generating Product Embeddings...\")\n",
    "unique_products = data[['product_id', 'text', 'brand', 'category_code', 'price']].drop_duplicates('product_id')\n",
    "product_embeddings = get_bert_embeddings(unique_products['text'].tolist())\n",
    "\n",
    "# Map Product ID to Embedding\n",
    "product_id_to_embedding = {\n",
    "    pid: emb for pid, emb in zip(unique_products['product_id'], product_embeddings)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64c4198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02960af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7b54186",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_bert_embeddings(texts, tokenizer, model, batch_size=64, save_path=\"embeddings_chunk.npy\"):\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        batch = [str(x) for x in batch] \n",
    "\n",
    "        inputs = tokenizer(batch, padding=\"max_length\", truncation=True, max_length=32, return_tensors=\"tf\")\n",
    "        outputs = model(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        np_embeddings = cls_embeddings.numpy()\n",
    "\n",
    "        all_embeddings.append(np_embeddings)\n",
    "\n",
    "        # Save chunk periodically\n",
    "        if i % (batch_size * 10) == 0:\n",
    "            print(f\"Processed {i}/{len(texts)}\")\n",
    "            np.save(save_path, np.concatenate(all_embeddings, axis=0))\n",
    "\n",
    "    return tf.convert_to_tensor(np.concatenate(all_embeddings, axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ebd5d",
   "metadata": {},
   "source": [
    "## 4. BERT Embedding Generation\n",
    "We use a pre-trained `bert-base-uncased` model to convert product text descriptions into dense vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23823e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embeddings(texts, batch_size=64):\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Process in batches to manage memory\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = [str(x) for x in texts[i:i+batch_size]]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=32, \n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "        \n",
    "        outputs = bert_model(**inputs)\n",
    "        # Extract CLS token (contextual representation)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        all_embeddings.append(cls_embeddings.numpy())\n",
    "        \n",
    "        if i % (batch_size * 10) == 0:\n",
    "            print(f\"Processed {i}/{len(texts)}\", end='\\r')\n",
    "            \n",
    "    return np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "\n",
    "unique_products = data[['product_id', 'text', 'brand', 'category_code', 'price']].drop_duplicates('product_id')\n",
    "product_embeddings = get_bert_embeddings(unique_products['text'].tolist())\n",
    "\n",
    "# Map Product ID to Embedding\n",
    "product_id_to_embedding = {\n",
    "    pid: emb for pid, emb in zip(unique_products['product_id'], product_embeddings)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01313fc7",
   "metadata": {},
   "source": [
    "## 5. Model 1: User-Based Content Recommender\n",
    "**Logic:** A user's profile is the average vector of all products they have viewed or purchased. We recommend products closest to this \"User Vector.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26bd5c9",
   "metadata": {},
   "source": [
    "**Recommendation function (user â†’ top-K products)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def recommend_for_user(user_id, top_k):\n",
    "    if user_id not in user_id_to_profile:\n",
    "        print(\"No interaction data for this user.\")\n",
    "        return []\n",
    "\n",
    "    user_vec = user_id_to_profile[user_id].reshape(1, -1)\n",
    "    all_item_vecs = embeddings.numpy()\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    scores = cosine_similarity(user_vec, all_item_vecs).flatten()\n",
    "\n",
    "    # Top-K item indices\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "    # Return product_ids\n",
    "    return data.iloc[top_indices][['product_id', 'brand', 'category_code', 'price', 'text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b8d323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>brand</th>\n",
       "      <th>category_code</th>\n",
       "      <th>price</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29862</th>\n",
       "      <td>7900194</td>\n",
       "      <td>joie</td>\n",
       "      <td>furniture.kitchen.chair</td>\n",
       "      <td>73.10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2661</th>\n",
       "      <td>10900336</td>\n",
       "      <td>smeg</td>\n",
       "      <td>appliances.kitchen.mixer</td>\n",
       "      <td>544.53</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>1005130</td>\n",
       "      <td>apple</td>\n",
       "      <td>electronics.smartphone</td>\n",
       "      <td>1558.25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31918</th>\n",
       "      <td>1004777</td>\n",
       "      <td>xiaomi</td>\n",
       "      <td>electronics.smartphone</td>\n",
       "      <td>135.01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29298</th>\n",
       "      <td>1307478</td>\n",
       "      <td>lenovo</td>\n",
       "      <td>computers.notebook</td>\n",
       "      <td>252.23</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       product_id   brand             category_code    price text\n",
       "29862     7900194    joie   furniture.kitchen.chair    73.10  NaN\n",
       "2661     10900336    smeg  appliances.kitchen.mixer   544.53  NaN\n",
       "1609      1005130   apple    electronics.smartphone  1558.25  NaN\n",
       "31918     1004777  xiaomi    electronics.smartphone   135.01  NaN\n",
       "29298     1307478  lenovo        computers.notebook   252.23  NaN"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_for_user(513103710,top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c7ab26",
   "metadata": {},
   "source": [
    "## 6. Model 2: Brand & Price Based Recommender\n",
    "**Logic:** Retrieve products with similar embeddings to a query product, with optional filters for price ranges and categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db95b02",
   "metadata": {},
   "source": [
    "**Generate brand embeddings (same pattern as above)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4e35a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brand_embeddings(texts, tokenizer, model, batch_size=64, save_path=\"brand_embeddings_chunk.npy\"):\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        batch = [str(x) for x in batch] \n",
    "\n",
    "        inputs = tokenizer(batch, padding=\"max_length\", truncation=True, max_length=32, return_tensors=\"tf\")\n",
    "        outputs = model(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        np_embeddings = cls_embeddings.numpy()\n",
    "\n",
    "        all_embeddings.append(np_embeddings)\n",
    "\n",
    "        # Save chunk periodically\n",
    "        if i % (batch_size * 10) == 0:\n",
    "            print(f\"Processed {i}/{len(texts)}\")\n",
    "            np.save(save_path, np.concatenate(all_embeddings, axis=0))\n",
    "\n",
    "    return tf.convert_to_tensor(np.concatenate(all_embeddings, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "263e70bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0/46038\n",
      "Processed 640/46038\n",
      "Processed 1280/46038\n",
      "Processed 1920/46038\n",
      "Processed 2560/46038\n",
      "Processed 3200/46038\n",
      "Processed 3840/46038\n",
      "Processed 4480/46038\n",
      "Processed 5120/46038\n",
      "Processed 5760/46038\n",
      "Processed 6400/46038\n",
      "Processed 7040/46038\n",
      "Processed 7680/46038\n",
      "Processed 8320/46038\n",
      "Processed 8960/46038\n",
      "Processed 9600/46038\n",
      "Processed 10240/46038\n",
      "Processed 10880/46038\n",
      "Processed 11520/46038\n",
      "Processed 12160/46038\n",
      "Processed 12800/46038\n",
      "Processed 13440/46038\n",
      "Processed 14080/46038\n",
      "Processed 14720/46038\n",
      "Processed 15360/46038\n",
      "Processed 16000/46038\n",
      "Processed 16640/46038\n",
      "Processed 17280/46038\n",
      "Processed 17920/46038\n",
      "Processed 18560/46038\n",
      "Processed 19200/46038\n",
      "Processed 19840/46038\n",
      "Processed 20480/46038\n",
      "Processed 21120/46038\n",
      "Processed 21760/46038\n",
      "Processed 22400/46038\n",
      "Processed 23040/46038\n",
      "Processed 23680/46038\n",
      "Processed 24320/46038\n",
      "Processed 24960/46038\n",
      "Processed 25600/46038\n",
      "Processed 26240/46038\n",
      "Processed 26880/46038\n",
      "Processed 27520/46038\n",
      "Processed 28160/46038\n",
      "Processed 28800/46038\n",
      "Processed 29440/46038\n",
      "Processed 30080/46038\n",
      "Processed 30720/46038\n",
      "Processed 31360/46038\n",
      "Processed 32000/46038\n",
      "Processed 32640/46038\n",
      "Processed 33280/46038\n",
      "Processed 33920/46038\n",
      "Processed 34560/46038\n",
      "Processed 35200/46038\n",
      "Processed 35840/46038\n",
      "Processed 36480/46038\n",
      "Processed 37120/46038\n",
      "Processed 37760/46038\n",
      "Processed 38400/46038\n",
      "Processed 39040/46038\n",
      "Processed 39680/46038\n",
      "Processed 40320/46038\n",
      "Processed 40960/46038\n",
      "Processed 41600/46038\n",
      "Processed 42240/46038\n",
      "Processed 42880/46038\n",
      "Processed 43520/46038\n",
      "Processed 44160/46038\n",
      "Processed 44800/46038\n",
      "Processed 45440/46038\n"
     ]
    }
   ],
   "source": [
    "brand_embeddings = get_brand_embeddings(list(data1['brand_text']), tokenizer, bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61581cd5",
   "metadata": {},
   "source": [
    "**Brand similarity / filtering function**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ce26fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_brand_price_bert(\n",
    "    product_id=None,\n",
    "    brand=None,\n",
    "    top_k= None,\n",
    "    same_category=True,\n",
    "    min_price=None,\n",
    "    max_price=None\n",
    "):\n",
    "    if product_id is not None:\n",
    "        if product_id not in data1['product_id'].values:\n",
    "            print(\"Product not found.\")\n",
    "            return []\n",
    "\n",
    "        idx = data1.index.get_loc(data1.index[data1['product_id'] == product_id][0])\n",
    "    \n",
    "    elif brand is not None:\n",
    "        brand_matches = data1[data1['brand'].str.lower() == brand.lower()]\n",
    "        if brand_matches.empty:\n",
    "            print(\"Brand not found.\")\n",
    "            return []\n",
    "\n",
    "        rep_row = brand_matches.iloc[0]\n",
    "        idx = data1.index.get_loc(rep_row.name)\n",
    "        product_id = rep_row['product_id']\n",
    "    \n",
    "    else:\n",
    "        print(\"You must provide either a product_id or a brand.\")\n",
    "        return []\n",
    "\n",
    "    # Get product info\n",
    "    product = data1.iloc[idx]\n",
    "    product_vec = brand_embeddings[idx].numpy().reshape(1, -1)\n",
    "\n",
    "    # Filter the dataset\n",
    "    filtered_data = data1.copy()\n",
    "    if same_category:\n",
    "        filtered_data = filtered_data[filtered_data['category_code'] == product['category_code']]\n",
    "\n",
    "    # price range filter\n",
    "    if min_price is not None:\n",
    "        filtered_data = filtered_data[filtered_data['price'] >= min_price]\n",
    "    if max_price is not None:\n",
    "        filtered_data = filtered_data[filtered_data['price'] <= max_price]\n",
    "\n",
    " \n",
    "    filtered_data = filtered_data[filtered_data['product_id'] != product_id]\n",
    "\n",
    "    # Get filtered indices\n",
    "    filtered_indices = [data1.index.get_loc(i) for i in filtered_data.index]\n",
    "    if not filtered_indices:\n",
    "        print(\"No similar products found after filtering.\")\n",
    "        return []\n",
    "\n",
    "    # Compute similarity\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    filtered_embeddings = tf.gather(brand_embeddings, filtered_indices)\n",
    "    similarities = cosine_similarity(product_vec, filtered_embeddings.numpy()).flatten()\n",
    "\n",
    "    # Get top K\n",
    "    top_indices = similarities.argsort()[::-1][:top_k]\n",
    "    top_data_indices = [filtered_data.index[i] for i in top_indices]\n",
    "\n",
    "    return data1.loc[top_data_indices][['product_id', 'brand', 'category_code', 'price']]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b44424",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_similar_brand_price_bert( brand = \"wincars\", top_k=10, max_price= 13) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c070b39e",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "* **Performance:** BERT successfully captures semantic relationships between brands and categories (e.g., grouping high-end electronics).\n",
    "* **Utility:** The User-Based model allows for personalization, while the Brand-Based model supports exploration and \"similar item\" widgets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
