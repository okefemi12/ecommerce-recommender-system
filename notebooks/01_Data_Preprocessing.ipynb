{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e30778",
   "metadata": {},
   "source": [
    "# E-Commerce Data Preprocessing & Exploratory Data Analysis\n",
    "\n",
    "## 1. Project Overview\n",
    "This project aims to build a Multi-Model Recommendation System combining Content-Based, Collaborative, and Sequential approaches. This notebook focuses on the initial **Data Preprocessing and Exploratory Data Analysis (EDA)** steps.\n",
    "\n",
    "### Dataset Description\n",
    "The dataset is the **e-commerce clickstream dataset** from Kaggle: *\"Ecommerce Behavior Data from Multi-Category Store\"* (October & November 2019).\n",
    "* **Source:** [Kaggle Link](https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store)\n",
    "* **Scale:** Millions of rows representing user-product interactions.\n",
    "* **Key Features:** `user_id`, `product_id`, `event_type`, `brand`, `price`, `category_code`.\n",
    "\n",
    "## 2. Setup and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51170de8",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing Pipeline\n",
    "We apply the following steps to clean and reduce the dataset for efficient training:\n",
    "1.  **Data Cleaning:** Remove rows with missing critical values (`user_id`, `product_id`, `brand`, `category_code`).\n",
    "2.  **Filtering:** Keep only `view` and `purchase` events.\n",
    "3.  **Sparsity Reduction:** Retain only the top **50,000 users** and **10,000 products** to ensure the model learns from significant interactions.\n",
    "4.  **Sampling:** Downsample to 20% of the data for manageable training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf1cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_chunk(df):\n",
    "    \n",
    "    # 1. Drop missing critical columns\n",
    "    df = df.dropna(subset=['user_id', 'product_id', 'event_type', 'brand', 'price', 'category_code'])\n",
    "    \n",
    "    # 2. Deduplicate user-product interaction types\n",
    "    df = df.drop_duplicates(subset=['user_id', 'product_id', 'event_type'])\n",
    "\n",
    "    # 3. Filter event types\n",
    "    df = df[df['event_type'].isin(['view', 'purchase'])]\n",
    "\n",
    "    # 4. Filter for Top Users and Products (Simplified for chunking)\n",
    "    \n",
    "    \n",
    "    # 5. Type conversion\n",
    "    df['user_id'] = df['user_id'].astype(str)\n",
    "    df['product_id'] = df['product_id'].astype(str)\n",
    "    \n",
    "    # 6. Sampling (20%)\n",
    "    df = df.sample(frac=0.2, random_state=42)\n",
    "    \n",
    "    return df[['user_id', 'product_id', 'event_type', 'brand', 'price', 'category_code']]\n",
    "\n",
    "# Processing October Data\n",
    "chunk_size = 300_000\n",
    "print(\"Processing October Data...\")\n",
    "for i, chunk in enumerate(pd.read_csv('2019-Oct.csv', chunksize=chunk_size)):\n",
    "    print(f\"Processing chunk {i}...\", end='\\r')\n",
    "    processed_chunk = preprocess_chunk(chunk)\n",
    "    mode = 'w' if i == 0 else 'a'\n",
    "    header = i == 0\n",
    "    processed_chunk.to_csv('processed_data1.csv', mode=mode, header=header, index=False)\n",
    "\n",
    "print(\"\\nProcessing November Data...\")\n",
    "for i, chunk in enumerate(pd.read_csv('2019-Nov.csv', chunksize=chunk_size)):\n",
    "    print(f\"Processing chunk {i}...\", end='\\r')\n",
    "    processed_chunk = preprocess_chunk(chunk)\n",
    "    mode = 'w' if i == 0 else 'a'\n",
    "    header = i == 0\n",
    "    processed_chunk.to_csv('processed_data.csv', mode=mode, header=header, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc993f5d",
   "metadata": {},
   "source": [
    "## 4. Final Aggregation & EDA\n",
    "Combine the processed files and verify the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee2503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed chunks\n",
    "data_oct = pd.read_csv('processed_data.csv')\n",
    "data_nov = pd.read_csv('processed_data1.csv')\n",
    "full_data = pd.concat([data_oct, data_nov], ignore_index=True)\n",
    "\n",
    "# Save the final Master Dataset for all models\n",
    "full_data.to_csv(\"Ecommerce_Dataset.csv\", index=False)\n",
    "\n",
    "print(f\"Final Dataset Shape: {full_data.shape}\")\n",
    "display(full_data.head())\n",
    "display(full_data.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
