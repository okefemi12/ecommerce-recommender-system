{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hybrid Recommender System: Transformer-DQN Agent\n",
        "\n",
        "## 1. Objective\n",
        "The goal of this notebook is to build a **Production-Grade Reinforcement Learning Agent** that optimizes user engagement over time.\n",
        "\n",
        "While the previous models (Content-Based, Collaborative, Sequential) excel at *pattern matching* on static data, they lack the ability to **strategize**. This Hybrid Agent combines:\n",
        "1.  **Transformer Encoder:** To understand the sequential context of user history.\n",
        "2.  **Deep Q-Network (DQN):** To make decisions that maximize long-term cumulative reward (e.g., prioritizing a Purchase over a Click).\n",
        "3.  **Sim-to-Real Workflow:** Trained in a simulated environment to learn robust policies before deployment.\n",
        "\n",
        "### Architecture\n",
        "\n",
        "* **Input:** User History Sequence (Product IDs).\n",
        "* **Encoder:** Self-Attention mechanism to extract user intent.\n",
        "* **Head:** Dense layers predicting Q-Values (Expected Future Reward) for all 10 candidate products.\n",
        "\n",
        "## 2. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "De1QoFltUE3d"
      },
      "outputs": [],
      "source": [
        "%pip install gymnasium numpy pandas tensorflow mlflow dagshub tf-keras --ignore-installed blinker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B91d7v04UPXB"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense, LayerNormalization, Dropout,\n",
        "    MultiHeadAttention, Embedding,\n",
        "    GlobalAveragePooling1D, Input\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from collections import deque\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import dagshub\n",
        "import mlflow\n",
        "import dagshub.auth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygzf2904UUhN"
      },
      "outputs": [],
      "source": [
        "DAGSHUB_REPO_OWNER = \"dagshub_name\"\n",
        "DAGSHUB_REPO_NAME = \"dagshub_name\"\n",
        "DAGSHUB_TOKEN = \"dagshub_token\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayQjxI8IUYue"
      },
      "outputs": [],
      "source": [
        "my_token =DAGSHUB_TOKEN\n",
        "\n",
        "dagshub.auth.add_app_token(my_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "ziqo_9t6Ued3",
        "outputId": "1c7b47c7-19e5-4707-9dca-8555703a16c8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as oke03940\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Accessing as oke03940\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"oke03940/hybrid-recommender\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"oke03940/hybrid-recommender\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository oke03940/hybrid-recommender initialized!\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Repository oke03940/hybrid-recommender initialized!\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dagshub.init(repo_owner=DAGSHUB_REPO_OWNER, repo_name=DAGSHUB_REPO_NAME , mlflow=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFMunshxUiMu",
        "outputId": "aac7e4e2-13f5-4f7a-d574-a9e8e8128cf7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='mlflow-artifacts:/df8beef13d304d758d051fac47ef1bab', creation_time=1767616167081, experiment_id='0', last_update_time=1767616167081, lifecycle_stage='active', name='hybrid-recommender', tags={}>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlflow.set_experiment(\"hybrid-recommender\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. The Environment (User Simulator)\n",
        "We define a custom Gymnasium environment (`ECommerceEnv`) that simulates realistic user behavior.\n",
        "* **State:** The last 20 items the user viewed.\n",
        "* **Action:** Recommend 1 of 10 products.\n",
        "* **Reward:** * **Purchase:** +2.0\n",
        "    * **Click:** +1.0\n",
        "    * **View/No Action:** +0.0\n",
        "* **Hidden Logic:** The environment simulates \"Demographics\" and \"Interest Shifts\" that the agent must infer solely from the clickstream."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmLWAXjZX1m_"
      },
      "outputs": [],
      "source": [
        "class ECommerceEnv(gym.Env):\n",
        "    \n",
        "    metadata = {\"render_modes\": [\"human\"]}\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ECommerceEnv, self).__init__()\n",
        "\n",
        "        # Configuration\n",
        "        self.history_length = 20\n",
        "        self.vocab_size = 11  # 10 products + padding token\n",
        "\n",
        "        # Observation: Sequence of product IDs\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0,\n",
        "            high=self.vocab_size,\n",
        "            shape=(self.history_length,),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "        self.action_space = spaces.Discrete(10)\n",
        "\n",
        "        # Internal state (hidden from agent)\n",
        "        self.history = deque(maxlen=self.history_length)\n",
        "        self.state = None\n",
        "        self.steps = 0\n",
        "        self.max_steps = 20\n",
        "\n",
        "    def _generate_user_behavior(self):\n",
        "        \"\"\"Simulate user engagement metrics\"\"\"\n",
        "        purchases = np.random.randint(0, 11) / 10\n",
        "        clicks = np.random.randint(0, 21) / 20\n",
        "        return np.array([purchases, clicks], dtype=np.float32)\n",
        "\n",
        "    def _generate_user_demographics(self):\n",
        "        \"\"\"Simulate user profile features\"\"\"\n",
        "        age = np.random.rand(1)\n",
        "        gender = np.zeros(2)\n",
        "        gender[np.random.randint(0, 2)] = 1\n",
        "        location = np.zeros(5)\n",
        "        location[np.random.randint(0, 5)] = 1\n",
        "        return np.concatenate([age, gender, location])\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.steps = 0\n",
        "        self.history.clear()\n",
        "\n",
        "        # Generate hidden user profile\n",
        "        self.current_behavior = self._generate_user_behavior()\n",
        "        self.current_demographics = self._generate_user_demographics()\n",
        "\n",
        "        self._update_state()\n",
        "        return self.state, {}\n",
        "\n",
        "    def _update_state(self):\n",
        "        \"\"\"Convert history deque to padded array for Transformer\"\"\"\n",
        "        state = list(self.history)\n",
        "        while len(state) < self.history_length:\n",
        "            state.insert(0, 0)  # Left-pad with zeros\n",
        "        self.state = np.array(state, dtype=np.int32)\n",
        "\n",
        "    def step(self, action):\n",
        "        self.steps += 1\n",
        "        self.history.append(action)\n",
        "\n",
        "        # Calculate reward from implicit feedback\n",
        "        location_vector = self.current_demographics[3:]\n",
        "        location_index = np.argmax(location_vector)\n",
        "\n",
        "        click_prob = 0.1\n",
        "        if action == location_index:\n",
        "            click_prob = 0.4 + (0.5 * self.current_behavior[0])\n",
        "\n",
        "        clicked = np.random.rand() < click_prob\n",
        "\n",
        "        # Map implicit feedback to rewards\n",
        "        if clicked:\n",
        "            reward = self._calculate_reward(\"click\")\n",
        "            self.current_behavior[0] = min(1.0, self.current_behavior[0] + 0.1)\n",
        "            self.current_behavior[1] = min(1.0, self.current_behavior[1] + 0.05)\n",
        "        else:\n",
        "            reward = self._calculate_reward(\"no_action\")\n",
        "\n",
        "        self._update_state()\n",
        "        done = self.steps >= self.max_steps\n",
        "\n",
        "        return self.state, reward, done, False, {}\n",
        "\n",
        "    def _calculate_reward(self, event):\n",
        "        \n",
        "        reward_map = {\n",
        "            \"purchase\": 2.0,\n",
        "            \"click\": 1.0,\n",
        "            \"view\": 0.2,\n",
        "            \"no_action\": 0.0\n",
        "        }\n",
        "        return reward_map.get(event, 0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Architecture: Transformer-DQN\n",
        "We construct a state-of-the-art architecture.\n",
        "1.  **Embedding Layer:** Converts Item IDs to dense vectors.\n",
        "2.  **Transformer Block:** Uses Multi-Head Attention to capture the sequential relationship (e.g., A -> B -> C implies intent D).\n",
        "3.  **DQN Head:** A dense network that outputs the Q-Value for every possible action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY-VNEgCX8df"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"Multi-head self-attention block for sequence processing\"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__(**kwargs)\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation=\"relu\"),\n",
        "            Dense(embed_dim)\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "def build_hybrid_model(history_length, action_size, vocab_size=12):\n",
        "    \n",
        "    inputs = Input(shape=(history_length,), dtype=tf.int32)\n",
        "\n",
        "    # Transformer encoder\n",
        "    embedding = Embedding(vocab_size, 32)(inputs)\n",
        "    x = TransformerBlock(embed_dim=32, num_heads=2, ff_dim=64)(embedding)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "    # DQN head\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    outputs = Dense(action_size, activation='linear')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. The Agent\n",
        "The Agent manages the interaction loop.\n",
        "* **Replay Buffer:** Stores experiences (`state`, `action`, `reward`, `next_state`) to break temporal correlations during training.\n",
        "* **Epsilon-Greedy Strategy:** Balances exploration (trying new items) and exploitation (recommending best items).\n",
        "* **IPS Correction:** Prepares the agent for offline deployment by tracking behavior probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFP3h16qX--b"
      },
      "outputs": [],
      "source": [
        "class OfflineRLAgent:\n",
        "    \n",
        "\n",
        "    def __init__(self, state_size, action_size, model):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.model = model\n",
        "\n",
        "        # Replay buffer with behavior policy probabilities\n",
        "        self.memory = deque(maxlen=2000)\n",
        "\n",
        "        # Exploration parameters\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.05\n",
        "        self.epsilon_decay = 0.995\n",
        "\n",
        "        # Discount factor\n",
        "        self.gamma = 0.99\n",
        "\n",
        "        # Training mode flag\n",
        "        self.training = True\n",
        "\n",
        "    def behavior_policy(self, state, candidates):\n",
        "        \n",
        "        probs = np.ones(len(candidates)) / len(candidates)\n",
        "        action = np.random.choice(len(candidates), p=probs)\n",
        "        return action, probs[action]\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done, behavior_prob):\n",
        "        \n",
        "        self.memory.append({\n",
        "            'state': state,\n",
        "            'action': action,\n",
        "            'reward': reward,\n",
        "            'next_state': next_state,\n",
        "            'done': done,\n",
        "            'behavior_prob': behavior_prob\n",
        "        })\n",
        "\n",
        "    def act(self, state, candidates=None):\n",
        "        \n",
        "        if candidates is None:\n",
        "            candidates = list(range(self.action_size))\n",
        "\n",
        "        # Exploration\n",
        "        if self.training and np.random.rand() <= self.epsilon:\n",
        "            action = random.choice(candidates)\n",
        "            behavior_prob = 1.0 / len(candidates)\n",
        "            return action, behavior_prob\n",
        "\n",
        "        # Exploitation: Candidate re-ranking\n",
        "        q_values = self.model.predict(state, verbose=0)[0]\n",
        "        candidate_qs = [(idx, q_values[idx]) for idx in candidates]\n",
        "        action = max(candidate_qs, key=lambda x: x[1])[0]\n",
        "\n",
        "        # Compute behavior probability for logging\n",
        "        _, behavior_prob = self.behavior_policy(state, candidates)\n",
        "\n",
        "        return action, behavior_prob\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        \n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        # Sample minibatch\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "\n",
        "        # Vectorized data preparation\n",
        "        states = np.array([t['state'] for t in minibatch])\n",
        "        states = np.squeeze(states)\n",
        "\n",
        "        actions = np.array([t['action'] for t in minibatch])\n",
        "        rewards = np.array([t['reward'] for t in minibatch])\n",
        "        next_states = np.array([t['next_state'] for t in minibatch])\n",
        "        next_states = np.squeeze(next_states)\n",
        "        dones = np.array([t['done'] for t in minibatch])\n",
        "        behavior_probs = np.array([t['behavior_prob'] for t in minibatch])\n",
        "\n",
        "        # Predict Q-values\n",
        "        targets = self.model.predict(states, verbose=0)\n",
        "        next_q_values = self.model.predict(next_states, verbose=0)\n",
        "\n",
        "        # Compute IPS weights with clipping\n",
        "        ips_weights = 1.0 / np.maximum(behavior_probs, 0.01)\n",
        "        ips_weights = np.minimum(ips_weights, 10.0)  # Safety clipping\n",
        "\n",
        "        # Bellman update with IPS correction\n",
        "        for i in range(batch_size):\n",
        "            target_val = rewards[i]\n",
        "            if not dones[i]:\n",
        "                target_val = rewards[i] + self.gamma * np.amax(next_q_values[i])\n",
        "\n",
        "            # Apply IPS weight to TD error\n",
        "            targets[i][actions[i]] = target_val\n",
        "\n",
        "        # Weighted training\n",
        "        sample_weights = ips_weights\n",
        "        self.model.fit(\n",
        "            states,\n",
        "            targets,\n",
        "            sample_weight=sample_weights,\n",
        "            epochs=1,\n",
        "            verbose=0,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "\n",
        "    def log_interaction(self, state, action, reward, behavior_prob, q_value):\n",
        "        \n",
        "        log_entry = {\n",
        "            \"state_hash\": hash(state.tobytes()),\n",
        "            \"action\": int(action),\n",
        "            \"reward\": float(reward),\n",
        "            \"behavior_prob\": float(behavior_prob),\n",
        "            \"q_value\": float(q_value),\n",
        "            \"epsilon\": float(self.epsilon)\n",
        "        }\n",
        "        return log_entry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Loop (Sim-to-Real)\n",
        "We run the simulation for **700 episodes**.\n",
        "* **Tracking:** All metrics (Reward, Epsilon) are logged to **MLflow/DagsHub** in real-time.\n",
        "* **Convergence:** We expect the \"Reward\" to increase and stabilize as \"Epsilon\" decays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fzoCCNjYHUA",
        "outputId": "170c25d3-faa0-47a5-a0d7-0dfad9cadaa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building model: Input=20, Output=10\n"
          ]
        }
      ],
      "source": [
        "env = ECommerceEnv()\n",
        "\n",
        "# !!! FIX: Explicitly cast to Python int to prevent Keras ValueError !!!\n",
        "state_size = int(env.history_length)\n",
        "action_size = int(env.action_space.n)\n",
        "\n",
        "# Build model\n",
        "print(f\"Building model: Input={state_size}, Output={action_size}\")\n",
        "model = build_hybrid_model(state_size, action_size)\n",
        "agent = OfflineRLAgent(state_size, action_size, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AK0cBklEZ2Xo"
      },
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name=\"Transformer_DQN_Final\"):\n",
        "\n",
        "    params = {\"episodes\": 700, \"batch_size\": 32}\n",
        "    mlflow.log_params(params)\n",
        "    print(\"\\n=== Starting Training with MLflow ===\\n\")\n",
        "\n",
        "    for episode in range(params[\"episodes\"]):\n",
        "        state, _ = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "        episode_reward = 0\n",
        "\n",
        "        for step in range(200):\n",
        "            action, behavior_prob = agent.act(state)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "\n",
        "            agent.remember(state, action, reward, next_state, done, behavior_prob)\n",
        "\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            agent.replay(params[\"batch_size\"])\n",
        "\n",
        "            if done: break\n",
        "            # Decay exploration\n",
        "        if agent.epsilon > agent.epsilon_min:\n",
        "            agent.epsilon *= agent.epsilon_decay\n",
        "\n",
        "\n",
        "        # Log metrics\n",
        "        mlflow.log_metric(\"reward\", episode_reward, step=episode)\n",
        "        mlflow.log_metric(\"epsilon\", agent.epsilon, step=episode)\n",
        "\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            print(f\"Episode {episode+1} | Reward: {episode_reward:.2f} | Epsilon: {agent.epsilon:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlflow.tensorflow.log_model(agent.model, \"model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(agent.model)\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS, \n",
        "    tf.lite.OpsSet.SELECT_TF_OPS \n",
        "]\n",
        "    \n",
        "    try:\n",
        "        tflite_model = converter.convert()\n",
        "        with open('hybrid_dqn_model.tflite', 'wb') as f:\n",
        "            f.write(tflite_model)\n",
        "        \n",
        "        mlflow.log_artifact('hybrid_dqn_model.tflite')\n",
        "        print(\"saved and uploaded to DagsHub.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Conclusion\n",
        "This notebook successfully demonstrated the engineering of a **Self-Learning Agent**:\n",
        "1.  **Simulated Reality:** Used `Gymnasium` to model user intent and implicit feedback.\n",
        "2.  **Hybrid Intelligence:** Combined Transformers (Sequence awareness) with RL (Long-term planning).\n",
        "3.  **Deployment Ready:** The final model is serialized to **TFLite**, ready to be embedded in the FastAPI backend."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
